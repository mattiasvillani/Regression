---
title: "Forward and Backward selection"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=4in,height=4in]{cat_variableselection.jpg}\LARGE\\}
  - \posttitle{\end{center}}
---

# Forward selection

Vid den s k **forward selection** metoden börjar man enbart ett intercept och lägger sedan till en variabel i taget. Vid varje steg lägger man till en ny variabel tills dess ingen ny variabel har en t-kvot som överstiger tröskeln $t_{obs} = 2$. Valet av just talet 2 som tröskelvärde är rätt godtyckligt, men innebär att man bara lägger till en variabel om den är signifikant på ungefär 5% signifikansnivå (det exakta kritiska värdet är kring 2, men beror ju på antalet frihetsgrader).

```{r, include = F}
library(regkurs)
```
\newpage

## Steg 1 - val av den första förklarande variabeln

Vi skattar enkla regressioner för var och en av den fem variablerna separat:

```{r}
lmfit = lm(nRides ~ temp, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ hum, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ windspeed, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ holiday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ workingday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)
```

Det största t-värdet i absolutbelopp (dvs bortsett från tecknet framför t-värdet) får vi för variabeln `temp` som har $t_{obs} = 21.7594$. Det t-värdet är också större än 2 (i absolutbelopp), så vi inkluderar därför `temp` i modellen. Notera att vi bryr oss inte om t-värdet för interceptet här. Interceptet är alltid med i modellen.

## Steg 2 - val av den andra förklarande variabeln

Nu fortsätter vi och lägger till en andra förklarande variabel, givet att `temp` redan är med i modellen. Vi skattar där nya regressioner där var och en har `temp` och ytterligare en variabel som förklarande variabler:

```{r}

lmfit = lm(nRides ~ temp + hum, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ temp + windspeed, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ temp + holiday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ temp + workingday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)
```
Givet att `temp` är med i modellen så har `hum` det största t-värdet i absolutbelopp; vi har $t_{obs} = -6.4789$ för `hum` i utskriften ovan. Eftersom detta t-värde är större än 2 (i absolutbelopp) så lägger vi även till variabeln `hum` i modellen. Vi har nu en modell med både `temp` och `hum` som förklarande variabler. Notera att det spelar ingen roll att `temp` har större t-värde än `hum`. Variabeln `temp` är ju redan vald i steg 1 och det är inte längre en fråga om den variabel ska vara med eller inte. Steg 2 här handlar enbart om valet bland de andra variablerna utöver `temp`. Vi fortsätter nu och ser om det är värt att lägga till en tredje variabel.

## Steg 3 - val av den tredje förklarande variabeln

Nu skattar vi regressioner med `temp`, `hum` och ytterligare en variabel som förklarande variabler:
```{r}

lmfit = lm(nRides ~ temp + hum + windspeed, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ temp + hum + holiday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ temp + hum + workingday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)
```
Vi lägger även till `windspeed` som förklarande variabel efter dess t-värde är störst i absolutebelopp i utskrifterna ovan och större än 2. 

\newpage

## Steg 4 - fjärde variabeln

Vi fortsätter och frågar oss om det är värt att lägga till någon ytterligare variabel utöver de redan valda `temp`, `hum` och `windspeed`. Vi skattar därför regressionerna:

```{r}

lmfit = lm(nRides ~ temp + hum + windspeed + holiday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)

lmfit = lm(nRides ~ temp + hum + windspeed + workingday, data = bike)
regsummary(lmfit, anova = F, fit_measures = F)
```
Variabeln `holiday` har $t_{obs} = -1.9470$ vilket är större i absolutbelopp än t-värdet för `workingday` (som är $t_{obs} = 1.1079$). `holiday` verkar därför vara en aningens bättre variabel än `workingday`. Men, $t_{obs} = -1.9470$ för `holiday` är mindre än 2 i absolutbelopp vilket innebär att `holiday` inte ska tas med i modellen. Vi stannar där för här och väljer modellen med variablerna `temp`, `hum` och `windspeed`:

```{r}
lmfit = lm(nRides ~ temp + hum + windspeed , data = bike)
regsummary(lmfit, anova = F)
```
\newpage 

# Backward selection

Backward selection metoden börjar med alla variabler i modellen och börjar sen ta bort icke-signifikanta variabler, en och en i taget. I varje steg tar vi bort den variabel som har lägst t-kvot, om den t-kvoten är mindre än 2 i absolutbelopp. Om den minsta t-kvoten är större än 2 i absolutbelopp drar vi slutsatsen att alla kvarvarande variabler behövs i modellen, och vi stannar då med den modellen som vårt slutgiltiga modell. Here we go:

## Steg 1 - kan någon variabel tas bort?

Vi skattar först modellen med alla fem förklarande variabler:

```{r}
lmfit = lm(nRides ~ temp + hum + windspeed + holiday + workingday , data = bike)
regsummary(lmfit)
```

Vi ser från utskriften att variabeln `workingday` har lägst t-kvot (i absolutebelopp, dvs $|0.63994| < |-1.72236|$). Eftersom och eftersom t-kvoten för `workingday` är mindre än 2 i absolutbelopp så kastar vi ut denna variabel från modellen, och fortsätter med variablerna `temp`, `hum`, `windspeed` och `holiday` till nästa steg.

\newpage

## Steg 2 - kan ytterligare en variabel tas bort?
```{r}
lmfit = lm(nRides ~ temp + hum + windspeed + holiday, data = bike)
regsummary(lmfit)
```

Vi ser att `holiday` har lägst t-kvot i absolutbelopp, och att denna absoluta t-kvot är mindre än 2. Även `holiday` åker ut ur modellen. Vi fortsätter och ser om vi ska göra oss med ytterligare en variabel.

\newpage

## Steg 3 - kan vi göra oss av med ytterligare en variabel?
```{r}
lmfit = lm(nRides ~ temp + hum + windspeed, data = bike)
regsummary(lmfit)
```
Variabeln `windspeed` har lägst t-kvot i absolutbelopp, men $|-6.7808|=6.7808$ är större än 2, så vi vill inte ta bort `windspeed` från modellen. Vi stannar alltså här och vår slutliga modell blir alltså modellen med `temp`, `hum` och `windspeed`. 

I det här exemplet gav forward och backward selection samma slutliga modell. Så är dock inte alltid.

